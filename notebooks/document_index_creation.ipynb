{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumuk\\anaconda3\\envs\\ResearchSurveyLLM\\lib\\site-packages\\llama_index\\vector_stores\\lancedb.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  from pandas import DataFrame\n",
      "c:\\Users\\sumuk\\anaconda3\\envs\\ResearchSurveyLLM\\lib\\site-packages\\llama_index\\download\\module.py:12: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import json\n",
    "import arxiv\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "from llama_index import VectorStoreIndex, ServiceContext\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index import Document\n",
    "from llama_index.vector_stores import WeaviateVectorStore\n",
    "from llama_index import VectorStoreIndex, StorageContext\n",
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "from llama_index.postprocessor import MetadataReplacementPostProcessor, SentenceTransformerRerank\n",
    "from llama_index.response.notebook_utils import display_response\n",
    "from llama_index.readers.file.docs_reader import PDFReader\n",
    "from llama_index import load_index_from_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"20220101000000\" \n",
    "end_date = \"20240131235959\"    \n",
    "topic = \"Denoising Diffusion models\"\n",
    "max_count = 5\n",
    "required_metadata = [\"authors\", \"title\", \"summary\", \"published\"]\n",
    "temp_save_folder = r\"C:\\Users\\sumuk\\OneDrive\\Desktop\\GitHub\\ResearchSurveyLLM\\delete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(paper_object):\n",
    "    metadata = dict()\n",
    "    for i in required_metadata:\n",
    "        if i == \"authors\":\n",
    "            authors = \", \".join([j.name for j in getattr(paper_object, i)])\n",
    "            metadata[i] = authors\n",
    "        elif i == \"published\":\n",
    "            metadata[i] = paper_object.published.ctime()\n",
    "        else:\n",
    "            metadata[i] = str(getattr(paper_object, i)).replace(\"\\n\", \" \")\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = arxiv.Client(num_retries=10, page_size=500)\n",
    "search_query = arxiv.Search(query = topic,\n",
    "                            sort_by = arxiv.SortCriterion.Relevance\n",
    "                            )\n",
    "start_date = datetime.strptime(start_date, \"%Y%m%d%H%M%S\")\n",
    "end_date = datetime.strptime(end_date, \"%Y%m%d%H%M%S\")\n",
    "results = client.results(search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:43,  8.74s/it]\n"
     ]
    }
   ],
   "source": [
    "paper_arxiv_objects = []\n",
    "current_paper_count = 0\n",
    "for r in tqdm(results):\n",
    "    if start_date.timestamp() <= r.published.timestamp() <= end_date.timestamp():\n",
    "        current_paper_count += 1\n",
    "        # print(r.title)\n",
    "        r.download_pdf(temp_save_folder, f\"paper_{current_paper_count}\" + \".pdf\")\n",
    "        paper = PDFReader().load_data(os.path.join(temp_save_folder, f\"paper_{current_paper_count}\" + \".pdf\"))\n",
    "        paper = Document(text=\"\\n\\n\".join([doc.text for doc in paper]), metadata=get_metadata(r))\n",
    "        paper_arxiv_objects.append(paper)        \n",
    "        if current_paper_count == max_count:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumuk\\anaconda3\\envs\\ResearchSurveyLLM\\lib\\site-packages\\weaviate\\warnings.py:158: DeprecationWarning: Dep016: You are using the Weaviate v3 client, which is deprecated.\n",
      "            Consider upgrading to the new and improved v4 client instead!\n",
      "            See here for usage: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "client = weaviate.Client(\n",
    "    url = \"http://localhost:8080\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnexpectedStatusCodeError",
     "evalue": "Create class! Unexpected status code: 422, with response body: {'error': [{'message': 'class name \"ArXivPaper\" already exists'}]}.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnexpectedStatusCodeError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 41\u001b[0m\n\u001b[0;32m      1\u001b[0m schema \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m      3\u001b[0m                 {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m             ]\n\u001b[0;32m     40\u001b[0m          }\n\u001b[1;32m---> 41\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sumuk\\anaconda3\\envs\\ResearchSurveyLLM\\lib\\site-packages\\weaviate\\schema\\crud_schema.py:201\u001b[0m, in \u001b[0;36mSchema.create\u001b[1;34m(self, schema)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03mCreate the schema of the Weaviate instance, with all classes at once.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m    If the 'schema' could not be validated against the standard format.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m loaded_schema \u001b[38;5;241m=\u001b[39m _get_dict_from_object(schema)\n\u001b[1;32m--> 201\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_classes_with_primitives\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_schema\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclasses\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_complex_properties_from_classes(loaded_schema[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\sumuk\\anaconda3\\envs\\ResearchSurveyLLM\\lib\\site-packages\\weaviate\\schema\\crud_schema.py:832\u001b[0m, in \u001b[0;36mSchema._create_classes_with_primitives\u001b[1;34m(self, schema_classes_list)\u001b[0m\n\u001b[0;32m    820\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;124;03mCreate all the classes in the list and primitive properties.\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;124;03mThis function does not create references,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;124;03m    A list of classes as they are found in a schema JSON description.\u001b[39;00m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m weaviate_class \u001b[38;5;129;01min\u001b[39;00m schema_classes_list:\n\u001b[1;32m--> 832\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_class_with_primitives\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweaviate_class\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sumuk\\anaconda3\\envs\\ResearchSurveyLLM\\lib\\site-packages\\weaviate\\schema\\crud_schema.py:817\u001b[0m, in \u001b[0;36mSchema._create_class_with_primitives\u001b[1;34m(self, weaviate_class)\u001b[0m\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsConnectionError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass may not have been created properly.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconn_err\u001b[39;00m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m--> 817\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedStatusCodeException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate class\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n",
      "\u001b[1;31mUnexpectedStatusCodeError\u001b[0m: Create class! Unexpected status code: 422, with response body: {'error': [{'message': 'class name \"ArXivPaper\" already exists'}]}."
     ]
    }
   ],
   "source": [
    "schema = {\n",
    "            \"classes\": [\n",
    "                {\n",
    "                \"class\": \"ArXivPaper\",\n",
    "                \"description\": \"Represents a paper from ArXiv\",\n",
    "                \"properties\": [\n",
    "                    {\n",
    "                    \"name\": \"title\",\n",
    "                    \"dataType\": [\"string\"],\n",
    "                    \"description\": \"Title of the paper\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"name\": \"authors\",\n",
    "                    \"dataType\": [\"string\"],\n",
    "                    \"description\": \"Authors of the paper\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"name\": \"abstract\",\n",
    "                    \"dataType\": [\"string\"],\n",
    "                    \"description\": \"Abstract of the paper\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"name\": \"content\",\n",
    "                    \"dataType\": [\"text\"],\n",
    "                    \"description\": \"Full text content of the paper\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"name\": \"url\",\n",
    "                    \"dataType\": [\"string\"],\n",
    "                    \"description\": \"URL of the paper on ArXiv\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"name\": \"published_date\",\n",
    "                    \"dataType\": [\"date\"],\n",
    "                    \"description\": \"Date when the paper was published on ArXiv\"\n",
    "                    }\n",
    "                ]\n",
    "                }\n",
    "            ]\n",
    "         }\n",
    "client.schema.create(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_build_index(documents, embed_model=r\"local:C:\\Users\\sumuk\\OneDrive\\Desktop\\GitHub\\ResearchSurveyLLM\\UAE-Large-V1\", sentence_window_size=3,save_dir=\"C:/Users/sumuk/OneDrive/Desktop/GitHub/ResearchSurveyLLM/delete/index\"):\n",
    "  node_parser = SentenceWindowNodeParser(\n",
    "      window_size = sentence_window_size,\n",
    "      window_metadata_key = \"window\",\n",
    "      original_text_metadata_key = \"text\"\n",
    "  )\n",
    "\n",
    "  vector_store = WeaviateVectorStore(weaviate_client = client, index_name=\"ArXivPaper\", text_key=\"content\")\n",
    "  storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
    "\n",
    "\n",
    "  sentence_context = ServiceContext.from_defaults(\n",
    "      llm = None,\n",
    "      embed_model= embed_model,\n",
    "      node_parser = node_parser,\n",
    "  )\n",
    "\n",
    "  if not os.path.exists(save_dir):\n",
    "        # create and load the index\n",
    "        index = VectorStoreIndex.from_documents(\n",
    "            documents, service_context=sentence_context, storage_context = storage_context\n",
    "        )\n",
    "        index.storage_context.persist(persist_dir=save_dir)\n",
    "  else:\n",
    "      # load the existing index\n",
    "      index = load_index_from_storage(\n",
    "          StorageContext.from_defaults(persist_dir=save_dir),\n",
    "          service_context=sentence_context,\n",
    "      )\n",
    "\n",
    "  return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='789bd353-59b8-4650-9469-f6b296c4efa5', embedding=None, metadata={'authors': 'Zhennan Chen, Rongrong Gao, Tian-Zhu Xiang, Fan Lin', 'title': 'Diffusion Model for Camouflaged Object Detection', 'summary': 'Camouflaged object detection is a challenging task that aims to identify objects that are highly similar to their background. Due to the powerful noise-to-image denoising capability of denoising diffusion models, in this paper, we propose a diffusion-based framework for camouflaged object detection, termed diffCOD, a new framework that considers the camouflaged object segmentation task as a denoising diffusion process from noisy masks to object masks. Specifically, the object mask diffuses from the ground-truth masks to a random distribution, and the designed model learns to reverse this noising process. To strengthen the denoising learning, the input image prior is encoded and integrated into the denoising diffusion model to guide the diffusion process. Furthermore, we design an injection attention module (IAM) to interact conditional semantic features extracted from the image with the diffusion noise embedding via the cross-attention mechanism to enhance denoising learning. Extensive experiments on four widely used COD benchmark datasets demonstrate that the proposed method achieves favorable performance compared to the existing 11 state-of-the-art methods, especially in the detailed texture segmentation of camouflaged objects. Our code will be made publicly available at: https://github.com/ZNan-Chen/diffCOD.', 'published': 'Tue Aug  1 05:50:33 2023'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Diffusion Model for Camouflaged Object Detection\\nZhennan Chena, Rongrong Gaob, Tian-Zhu Xiangc;‚àóand Fan Lina;*\\naSchool of Informatics, Xiamen University, Xiamen, China\\nbDepartment of Computer Science and Engineering, HKUST, Hong Kong, China\\ncG42, Abu Dhabi, UAE\\nAbstract. Camouflaged object detection is a challenging task that\\naims to identify objects that are highly similar to their background.\\nDue to the powerful noise-to-image denoising capability of denois-\\ning diffusion models, in this paper, we propose a diffusion-based\\nframework for camouflaged object detection, termed diffCOD, a\\nnew framework that considers the camouflaged object segmenta-\\ntion task as a denoising diffusion process from noisy masks to ob-\\nject masks. Specifically, the object mask diffuses from the ground-\\ntruth masks to a random distribution, and the designed model learns\\nto reverse this noising process. To strengthen the denoising learn-\\ning, the input image prior is encoded and integrated into the de-\\nnoising diffusion model to guide the diffusion process. Furthermore,\\nwe design an injection attention module (IAM) to interact con-\\nditional semantic features extracted from the image with the dif-\\nfusion noise embedding via the cross-attention mechanism to en-\\nhance denoising learning. Extensive experiments on four widely used\\nCOD benchmark datasets demonstrate that the proposed method\\nachieves favorable performance compared to the existing 11 state-\\nof-the-art methods, especially in the detailed texture segmentation of\\ncamouflaged objects. Our code will be made publicly available at:\\nhttps://github.com/ZNan-Chen/diffCOD.\\n1 Introduction\\nCamouflage is to use any combination of coloration, illumination, or\\nmaterials to hide organisms in their surroundings, or disguise them as\\nsomething else, for deception and paralysis purposes. Camouflaged\\nobject detection (COD) [13], that is, segmenting camouflaged objects\\nfrom the background, is a challenging vision topic that has emerged\\nin recent years, due to the high similarity of camouflaged objects to\\nthe background. COD has also attracted growing research interest\\nfrom the computer vision community, because of its wide range of\\nreal-world applications, such as agricultural pest detection [30], med-\\nical image segmentation [34], and industrial defect detection [51].\\nWith the advent of large-scale camouflaged object detection\\ndatasets in recent years, such as CAMO [31] and COD10K [13]\\ndatasets, numerous deep learning-based methods have been proposed\\nand achieved great progress. Some methods are inspired by human\\nvisual mechanisms and adopt convolutional neural networks to imi-\\ntate predation behavior, thus designing a series of models for COD,\\nsuch as search identification network [12], positioning and focus net-\\nwork [37], zoom in and out [41], and PreyNet [61]. Some methods\\nadopt auxiliary cues to improve network discrimination, or branch\\n‚àóCorresponding Authors. Email: tianzhu.xiang19@gmail.com; ia-\\nmafan@xmu.edu.cn.\\nimage Network prediction\\n(a) Mainstream COD paradigm.\\nimage ùíöùëª ùëªDiffusion step prediction\\n(b) Diffusion-based COD paradigm.\\nFigure 1: (a) The current mainstream COD paradigm inputs images\\ninto the network for prediction in a single direction, generating a de-\\nterministic segmentation mask. (b) Our proposed diffCOD provides\\na novel paradigm that decomposes COD into a series of forward-and-\\nreverse diffusion processes.\\ntasks to jointly learn camouflage features. The former typically em-\\nploy frequency domain [63], edge/texture [24, 65], or motion infor-\\nmation [5] to improve feature representation, and the latter usually\\nintroduces boundary detection [50], classification [31], fixation [36],\\nor saliency detection [32] for multi-task collaborative learning. More\\nrecently, to improve global contextual exploration, transformer-based\\napproaches have also been proposed, such as HitNet [22] and FSP-\\nNet [23]. Although these methods have greatly improved the per-\\nformance of camouflaged object detection, the existing methods still\\nstruggle to achieve accurate location and segmentation in most com-\\nplex scenarios, due to the interference of highly similar backgrounds\\nand the complexity of the appearance of camouflaged objects.\\nIn recent years, diffusion models [20] have demonstrated im-\\npressive performance in the generative modeling of images and\\nvideos [10], opening up a new era of generative models. Diffusion\\nmodels are a class of generative models that consist of Markov chains\\ntrained using variational inference, to denoise noisy images blurred\\nby Gaussian noise via learning the reverse diffusion process. Be-\\ncause of its powerful noise-to-image denoising pipeline, the com-\\nputer vision community is curious about its variants for discrimina-\\ntive tasks [8]. More recently, diffusion models have been found to be\\nhighly effective in other computer vision tasks, such as image edit-\\ning [19], super-resolution [33], instance segmentation [17], semantic\\nsegmentation [3, 4] and medical image segmentation [43, 53]. How-\\never, despite their great potential, diffusion models for challenging\\ncamouflaged object detection have still not been well explored.\\nIn this paper, we propose to formulate the camouflaged object de-arXiv:2308.00303v2  [cs.CV]  5 Aug 2023\\n\\ntection as a generative task, through a denoising diffusion process\\nfrom the noisy mask to the object mask in the image. Specifically, in\\nthe training stage, Gaussian noise is added to the ground-truth masks\\nto obtain noisy masks, and then the model learns to reverse this nois-\\ning process. In the inference stage, the model progressively refines\\na set of randomly generated noisy masks from the image through\\nthe learned denoising model, until they perfectly cover the targeted\\nobject without noise. We can see that the denoising diffusion model\\nis the process of recovering the ground-truth mask from the random\\nnoisy distribution to the learned distribution over object masks. As\\nshown in Figure 1, unlike previous deterministic network solutions\\nthat produce a single output for an input image, we decouple the de-\\ntection of the object into a novel noise-to-mask paradigm with a se-\\nries of forward-and-reverse diffusion steps, which can output masks\\nfrom single or multi-step denoising, thereby generating multiple ob-\\nject segmentation masks from a single input image.\\nTo this end, we propose a denoising diffusion-based model, termed\\ndiffCOD, which approaches camouflaged object tasks from the per-\\nspective of the noise-to-mask denoising diffusion process. The pro-\\nposed model adopts a denoising network conditioned on the input im-\\nage prior. The semantic features extracted from the image by a Trans-\\nformer encoder are integrated into the denoising diffusion model to\\nguide the diffusion process at each step. To effectively bridge the gap\\nbetween the diffusion noise embedding and the conditional semantic\\nfeatures, an injection attention module (IAM) is designed to enhance\\nthe denoising diffusion learning by aggregating conditional seman-\\ntic features and diffusion model encoder through a cross-attention\\nmechanism. Our contributions are summarized as follows:\\n‚Ä¢We extend the denoising diffusion models to the task of camou-\\nflaged object detection, and propose a diffusion-based object seg-\\nmentation model, called diffCOD, a novel framework that views\\ncamouflaged object detection as a denoising diffusion process\\nfrom noisy masks to object masks.\\n‚Ä¢We design an injection attention module (IAM) to model the in-\\nteraction between noise embeddings and image features. The pro-\\nposed module adopts the cross-attention mechanism to integrate\\nthe conditional semantic feature extracted from the image into the\\ndiffusion model encoder to guide and enhance denoising learning.\\n‚Ä¢Extensive quantitative and qualitative experiments demonstrate\\nthat the proposed diffCOD achieves superior performance over\\nthe recent 11 state-of-the-art (SOTA) methods by a large margin,\\nespecially in object detail texture segmentation, indicating the ef-\\nfectiveness of the proposed method.\\n2 Related Work\\n2.1 Camouflaged Object Detection\\nExisting COD methods [11, 12, 13] are based on a non-generative ap-\\nproach to segment the objects from the background. The approaches\\nin COD can be broadly categorized into the following strategies: a)\\nIntroducing additional cues to facilitate the exploration of camou-\\nflage features. BGNet [50] uses edge semantic information to en-\\nable the model to extract features that highlight the structure of the\\nobject and thus pinpoint the object boundary. TINet [65] designs a\\ntexture label to find boundaries and texture differences through pro-\\ngressive interactive guidance. FDCOD [63] incorporates frequency\\ndomain features into CNN models to better detect objects from the\\nbackground. DGNet [24] utilizes gradient edge information to fa-\\ncilitate the generation of contextual and texture features. b) Multi-\\ntask learning strategies are used to improve segmentation capabil-ities. ANet [31] proposed joint learning of classification and seg-\\nmentation tasks to help the model improve recognition accuracy.\\nUJSC [32] detects both salient and camouflaged objects to improve\\nthe model performance. Rank-Net [36] proposes to use the localiza-\\ntion model to find the obvious discriminative region of the camou-\\nflaged object, and the segmentation model to segment the full range\\nof the camouflaged object. c) Coarse-to-fine feature learning strat-\\negy is utilized to explore and integrate multi-scale features. Seg-\\nMaR [27] uses multi-stage detection to focus on the region where the\\ngoal is located. ZoomNet [40] learns multi-scale semantic informa-\\ntion through multi-scale integration and hierarchical hybrid strategies\\nto promote models that produce predictions with higher confidence.\\nPreyNet [61] imitates the predation process for stepwise aggregation\\nand calibration of features. PFNet [37] mimics nature‚Äôs predation\\nprocess by first locating potential targets from a global perspective\\nand then gradually refining the fuzzy regions. SINet [13] is designed\\nto improve segmentation performance by locating the object first and\\nthen differentiating the details. C2FNet [49] proposes to use global\\ncontextual information to fuse on high-level features in a cascading\\nmanner to obtain better performance. HitNet [22] and FSPNet [23]\\npropose to explore global context cues by transformers. In this pa-\\nper, we introduce generative models, i.e., denoising diffusion mod-\\nels, into the COD task to gradually refine the object masks from the\\nnoisy image, which achieve excellent performance, especially for ob-\\njects with fine textures.\\n2.2 Diffusion Model\\nThe diffusion model [20, 47] is a generative model that uses a for-\\nward Gaussian diffusion process to sample a noisy image, and then\\niteratively refines it using a backward generative process to ob-\\ntain a denoised image. Diffusion models have shown strong poten-\\ntial in several fields, such as image synthesis [10, 20], image edit-\\ning [19], and image super-resolution [9]. Moreover, the learning\\nprocess of diffusion models is able to capture high-level semantic\\ninformation that is valuable for segmentation tasks [3], which has\\nled to a growing interest in diffusion models for image segmenta-\\ntion including medical image segmentation [53, 54], semantic seg-\\nmentation [4, 26, 55, 57], and instance segmentation [1, 17]. Med-\\nSegDiff [53] proposes the first DPM-based medical segmentation\\nmodel, and MedSegDiff-V2 [54] further improves the performance\\nbased on it using transformer. DDeP [4] finds that pre-training a se-\\nmantic segmentation model as a denoising self-encoder is beneficial\\nfor performance improvement. DDP [26] designs a dense prediction\\nframework with stepwise denoising refinement guided by image fea-\\ntures. ODISE [57] combines a trained text image diffusion model\\nwith a discriminative model to achieve open-vocabulary panoptic\\nsegmentation. DiffuMask [55] uses a model for the automatic genera-\\ntion of image and pixel-level semantic annotations, and it also shows\\nsuperiority in open vocabulary segmentation. DiffusionInst [17] pro-\\nposes the first instance segmentation model based on a diffusion pro-\\ncess to achieve global instance mask reconstruction. Segdiff [1] uses\\na diffusion probabilistic approach to design an end-to-end segmen-\\ntation model that does not rely on a pre-trained backbone. However,\\nthere are no studies that demonstrate the effectiveness of diffusion\\nmodels in COD tasks. In this work, we present the first diffusion\\nmodel for the COD segmentation task.\\n\\nC IAMFF\\nQ\\nK\\nVQ\\nK\\nV‚Ä¶Q\\nK\\nVQ\\nK\\nV‚Ä¶ViT\\nDenoising UNet\\nLinear‚äïC ‚äï R Element-wise Addition Matrix Multiplication‚äó Reshape Concatenation\\nNormLinear\\nNormS S‚äóR\\nLinear\\nNorm\\nIAM\\nùíü‚Ñ±\\nùëÑ!ùêæ!ùëâ!ùëÉ\"ùëâ\"\\nùíöùíïùíôùíê\\nùíöùíï#ùüèœµ%ùë¶&, ùë°, ùë• \\'SSoftmaxFigure 2: Our proposed diffCOD framework for COD, which feeds a given image into a denoising diffusion model with UNet architecture\\nas the core component for denoising. An injection attention module (IAM) is designed to implicitly guide the diffusion process with the\\nconditional semantic features that have gone through the backbone and feature fusion module (FF), allowing the model to take full advantage\\nof the correspondence between image features and diffusion information.\\n3 Methodology\\nIn this section, we first review the diffusion model (Sec. 3.1). Then\\nwe introduce the architecture of diffCOD (Sec. 3.2). Finally, we\\ndescribe the specific process of training and inference of diffCOD\\n(Sec. 3.3 & Sec. 3.4).\\n3.1 Diffusion Model\\nThe diffusion probability model has reaped plenty of attention due\\nto its simple training process and excellent performance. It is mainly\\ndivided into forward process and reverse process. In the forward pro-\\ncess, noise is added to the target image to make it closer to the Gaus-\\nsian distribution. The reverse process learns to map the noise to the\\nreal image.\\nThe forward process refers to the gradual incorporation of Gaus-\\nsian noise with variance Œ≤t‚àà(0,1)into the original image x0‚àº\\np(x0)at time tuntil it converges to isotropic Gaussian distribution.\\nThe forward process is described by the formulation:\\nq(xt|xt‚àí1) =N\\x10\\nxt;p\\n1‚àíŒ≤txt‚àí1, Œ≤tI\\x11\\n(1)\\nwhere t‚àà[1, T]. We can obtain the latent variable xtdirectly by\\nusing x0by the following equation:\\nq(xt|x0) =N\\x00\\nxt;‚àö¬ØŒ±tx0,(1‚àí¬ØŒ±t)I\\x01\\n(2)\\nwhere Œ±t:= 1‚àíŒ≤t,¬ØŒ±t:=Qt\\ns=0Œ±sandœµ‚àº N(0,I).\\nThe reverse process converts the latent variable distribution p(xT)\\ntop(x0)through a Markov chain, and the reverse process can be\\ndenoted as follows:\\npŒ∏(xt‚àí1|xt) =N(xt‚àí1;¬µŒ∏(xt, t),Œ£Œ∏(xt, t)) (3)\\nThe combination of qandpis a variational auto-encoder, and the\\nvariational lower bound (VLB) is defined as follows:\\nLvlb:=L0+L1+. . .+LT‚àí1+LT (4)L0:=‚àílogpŒ∏(x0|x1) (5)\\nLt‚àí1:=DKL(q(xt‚àí1|xt, x0)‚à•pŒ∏(xt‚àí1|xt)) (6)\\nLT:=DKL(q(xT|x0)‚à•p(xT)) (7)\\n3.2 Architecture\\nAs shown in Figure 2, the proposed diffCOD aims to solve the COD\\ntask by the diffusion model. The denoising network of diffCOD is\\nbased on the UNet architecture [44]. To get effective conditional se-\\nmantic features, we obtain multi-scale features by ViT-based back-\\nbone and feature fusion (FF) to yield features containing rich multi-\\nscale details. In addition, to let the texture patterns and localization\\ninformation in the conditional semantic features guide the denois-\\ning process, we propose an injection attention module (IAM) based\\non cross-attention. This allows the network to reduce the difference\\nbetween diffusion features and image features and to combine the\\nadvantages of both.\\nFeature Fusion (FF). Given an initial input image xo‚ààRH√óW√ó3,\\nwe adopt the top-three high-level features of the visual backbone as\\nour multi-scale backbone features, denoted as Xp\\ni,i‚àà {1,2,3}\\nwhose resolution isH\\nk√óW\\nk,k‚àà {8,16,32}. Here we use\\nPVTv2 [52] as the backbone. Then FF is used to aggregate these mul-\\ntiscale features. Specifically, FF contains three branches to process\\nXp\\ni, each branch uses two convolution operations with 3 √ó3 kernel\\nfor feature enhancement, and finally the three branches are coalesced\\nby a single convolution to obtain F ‚ààRH\\n32√óW\\n32√óC.\\nInjection Attention Module (IAM). To introduce texture and lo-\\ncation information of the original features in the noise prediction\\nprocess, we employ a cross-attention-based IAM, which is embed-\\nded in the middle of the UNet-based denoising network. Given the\\nmultiscale fusion feature Ffrom FF and the deepest feature D ‚àà\\nRH\\n32√óW\\n32√óCfrom the diffusion model as the common input to the\\nIAM. Specifically, Dis transformed by linear projection to generate\\n\\nthe query QD, the key KDand the value VD.Fgenerates PF,VF\\nby linear projection, and it is noteworthy that Fdoes not generate the\\nquery and the key for similarity comparison, but uses the generated\\nPFto act as an intermediary for similarity comparison with D. This\\nprocess is defined as follows:\\nQD=D ¬∑ WD\\nQ,KD=D ¬∑ WD\\nK,VD=D ¬∑ WD\\nV\\nPF=F ¬∑ WF\\nP,VF=F ¬∑ WF\\nV(8)\\nwhereWD\\nQ,WD\\nK,WD\\nV,WF\\nP,WF\\nV‚ààRd√ód.dis the dimensionality.\\nThus the IAM operation is defined as follows:\\nMatt\\n1= Softmax\\x12QD¬∑(PF)T\\n‚àö\\nd\\x13\\n(9)\\nMatt\\n2= Softmax\\x12KD¬∑(PF)T\\n‚àö\\nd\\x13\\n(10)\\nOI=Matt\\n1¬∑Matt\\n2¬∑(VD+VF) (11)\\nwhere Matt\\n1andMatt\\n2represent the attention maps of QD-PFand\\nKD-PF, respectively. OI‚ààRH\\n32√óW\\n32√óCdenotes the final generated\\ncross-attention fusion feature.\\n3.3 Training\\nIn the forward process, the Gaussian noise œµtis added to the ground\\ntruth y0to obtain the noise mapping yt‚àºq(yt|y0)byT-steps.\\nThe intensity of the noise is controlled by Œ±tand conforms to the\\nstandard normal distribution. This process can be defined as follows:\\nyt=‚àöŒ±tyt‚àí1+ (1‚àíŒ±t)œµt (12)\\nwhere t= [1,¬∑¬∑¬∑, T]andœµt‚àº N(0,I).\\nBy iterative computation, we can directly obtain yt. This process\\ncan be further marginalized as:\\nyt=‚àö¬ØŒ±ty0+ (1‚àí¬ØŒ±t)œµt (13)\\nwhere ¬ØŒ±t=Qt\\ni=1Œ±i.\\nIn the reverse process, we map from yttoyt‚àí1until the segmented\\nimage is acquired step by step. The mathematics is defined as fol-\\nlows:\\nyt‚àí1=¬µŒ∏(yt, t, x o) + Œ£ Œ∏(yt, t, x o)œµt (14)\\nWe train a denoising UNet model to predict œµŒ∏(yt, t, x o):\\n¬µŒ∏(yt, t, x o) =\\x10\\nyt‚àí\\x10\\n1‚àíŒ±t‚àö1‚àí¬ØŒ±t\\x11\\nœµŒ∏(yt, t, x o)\\x11\\n‚àöŒ±t(15)\\nWe follow the improved DDPM [39] to simplify Eq. (4)-(7) to\\ndefine the hybrid objective Lhybrid =Lsimple +Lvlb.Lvlblearns\\nthe term Œ£Œ∏(yt, t, x o). Furthermore, inspired by [54], we use FF and\\na convolution layer to provide an initial static mask ymto reduce the\\ndiffusion variance, and its mean square loss is defined as Lstatic .\\nTotal loss function Ltotal is defined as follows:\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3Lsimple =Et‚àº[1,T],y0‚àºq(y0),œµ‚à•œµ‚àíœµŒ∏(yt, t, x o)‚à•2\\nLstatic =Ey0‚àºq(y0),ym‚à•y0‚àíym‚à•2\\nLtotal =Lsimple +Lvlb+Lstatic(16)\\nAlgorithm 1 provides the training procedure for diffCOD.Algorithm 1: diffCOD Training\\ndef training_loss(images, masks):\\n\"\"\"images: [b, h, w, 3], masks: [b, h, w, 1]\"\"\"\\n# Encode images\\nX_p = ViT(images)\\nF = FF(X_p)\\n# corrupt groundtruth\\nt = uniform(0, 1)\\neps = normal(mean=0, std=1)\\nmask_crpt = sqrt(gamma(t)) *masks +\\nsqrt(1 - gamma(t)) *eps\\n# predict and backward\\nD = UNet_1(images, mask_crpt, t)\\nO = IAM(F, D)\\npreds = UNet_2(O)\\n# compute loss\\nloss = loss_function(preds, masks)\\nreturn loss\\n3.4 Inference\\nIn the inference stage, we step-by-step apply Eq. (14) to sample a\\npure Gaussian noise yt‚àº N(0, I). In addition, we add conditional\\ninformation related to the image features to guide the inference pro-\\ncess. After performing Titerations, we can obtain the segmentation\\nimage of the camouflaged object. Using the setting of [39] for the\\nsampling, the inference process of diffCOD is shown in Algorithm 2.\\nAlgorithm 2: diffCOD Inference\\ndef inference(images, steps):\\n\"\"\"images: [b, h, w, 3], steps: sample steps\"\"\"\\n# Encode images\\nX_p = ViT(images)\\nF = FF(X_p)\\nm_t = normal(mean=0, std=1)\\n# time intervals\\nfor step in range(steps):\\nout = p_sample(images, F, m_t, step)\\nreturn out\\n4 Experiments\\n4.1 Experimental Setup\\nDatasets. We conduct experiments on four widely used benchmark\\ndatasets of COD task, i.e., CAMO , CHAMELEON, COD10K and\\nNC4K. The details of each dataset are as follows:\\n‚Ä¢CAMO contains 1,250 camouflaged images and 1,250 non-\\ncamouflaged images, covering eight categories.\\n‚Ä¢CHAMELEON has a total of 76 camouflaged images.\\n‚Ä¢COD10K consists of 5,066 camouflaged, 1,934 non-camouflaged,\\nand 3,000 background images. It is currently the largest dataset\\nwhich covers 10 superclasses and 78 subclasses.\\n‚Ä¢NC4K is a newly published dataset that has a total of 4,121 cam-\\nouflaged images.\\n\\nMethodCOD10K NC4K CAMO CHAMELEON\\nSŒ±‚ÜëFœâ\\nŒ≤‚ÜëFm‚ÜëEm‚ÜëMAE ‚Üì SŒ± Fœâ\\nŒ≤‚ÜëFm‚ÜëEm‚ÜëMAE ‚Üì SŒ± Fœâ\\nŒ≤‚ÜëFm‚ÜëEm‚ÜëMAE ‚ÜìSŒ±‚ÜëFœâ\\nŒ≤‚ÜëFm‚ÜëEm‚ÜëMAE ‚Üì\\n2019 CPD [56] 0.736 0.547 0.607 0.801 0.053 0.769 0.652 0.713 0.822 0.072 0.688 0.552 0.623 0.728 0.114 0.876 0.809 0.821 0.914 0.036\\n2019 EGNet [62] 0.746 0.560 0.591 0.789 0.053 0.804 0.727 0.731 0.834 0.066 0.730 0.579 0.693 0.762 0.104 0.851 0.705 0.747 0.869 0.049\\n2020 SINet [13] 0.772 0.543 0.640 0.810 0.051 0.810 0.665 0.741 0.841 0.066 0.753 0.602 0.676 0.774 0.097 0.867 0.727 0.792 0.889 0.044\\n2020 MINet [41] 0.780 0.628 0.677 0.838 0.040 0.810 0.717 0.764 0.856 0.057 0.741 0.629 0.682 0.783 0.096 0.853 0.768 0.803 0.902 0.035\\n2020 PraNet [15] 0.800 0.656 0.699 0.869 0.041 0.826 0.739 0.780 0.878 0.056 0.769 0.664 0.716 0.812 0.091 0.870 0.790 0.816 0.915 0.039\\n2021 PFNet [37] 0.797 0.656 0.698 0.875 0.039 0.826 0.743 0.783 0.884 0.054 0.774 0.683 0.737 0.832 0.087 0.889 0.823 0.840 0.946 0.030\\n2021 LSR [36] 0.805 0.660 0.703 0.876 0.039 0.832 0.743 0.785 0.888 0.053 0.793 0.703 0.753 0.850 0.083 0.890 0.824 0.834 0.932 0.034\\n2022 ERRNet [25] 0.780 0.629 0.679 0.867 0.044 ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî 0.761 0.660 0.719 0.817 0.088 0.877 0.805 0.821 0.927 0.036\\n2022 NCHIT [60] 0.790 0.608 0.689 0.817 0.046 ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî 0.780 0.671 0.733 0.803 0.088 0.874 0.793 0.812 0.891 0.041\\n2022 CubeNet [66] 0.795 0.644 0.681 0.864 0.041 ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî 0.788 0.682 0.743 0.838 0.085 0.873 0.787 0.823 0.928 0.037\\n2023 CRNet [18] 0.733 0.576 0.627 0.832 0.049 ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî 0.735 0.641 0.702 0.815 0.092 0.818 0.744 0.756 0.897 0.046\\ndiffCOD 0.812 0.684 0.723 0.892 0.036 0.837 0.761 0.802 0.891 0.051 0.795 0.704 0.758 0.852 0.082 0.893 0.826 0.837 0.933 0.030\\nTable 1: Quantitative comparisons of our proposed method and other 11 state-of-the-art methods on four widely used benchmark datasets. The\\nhigher the SŒ±,Fœâ\\nŒ≤,Fm, andEm, the better the performance. The smaller the MAE , the better. The best results are marked in bold .\\nFollowing the standard practice of COD tasks, we use 3,040 im-\\nages from COD10K and 1,000 images from CAMO as the training\\nset and the remaining data as the test set.\\nEvaluation metrics. According to the standard evaluation proto-\\ncol of COD, we employ the five common metrics to evaluate our\\nmodel, i.e., structure-measure ( SŒ±), weighted F-measure ( Fœâ\\nŒ≤), mean\\nF-measure ( Fm), mean E-measure ( Em) and mean absolute error\\n(MAE ). The purpose of structure-measure ( SŒ±) is to evaluate the\\nstructural information of the result and ground truth, including ob-\\nject perception and region perception. Weighted F-measure Fœâ\\nŒ≤is the\\nweighted information of the mean F-measure ( Fm) metric, and these\\ntwo metrics are a combined assessment of the accuracy and recall of\\nthe result. Mean E-measure ( Em) is able to perform both pixel-level\\nmatching and image-level statistics, and is used to calculate the over-\\nall and local accuracy of the segmentation results. The mean absolute\\nerror ( MAE ) metric is often used to evaluate the average pixel-level\\nrelative error between the result and ground truth.\\nImplementation details. The proposed method is implemented with\\nthe PyTorch toolbox. We set the time step as T= 1000 with a linear\\nnoise schedule for all the experiments. We use Adam as our model\\noptimizer with a learning rate of 1e-4. The batch size is set to 64.\\nDuring the training, the input images are resized to 256 √ó256 via\\nbilinear interpolation and augmented by random flipping, cropping,\\nand color jittering.\\nBaselines. Our diffCOD is compared with 11 recent state-of-the-\\nart methods, including CPD [56], EGNet [62], SINet [13], MINet\\n[41], PraNet [15], PFNet [37], LSR [36], ERRNet [25], NCHIT [60],\\nCubeNet [66], CRNet [18]. For a fair comparison, all results are ei-\\nther provided by the authors or reproduced by an open-source model\\nre-trained on the same training set with the recommended setting.\\n4.2 Quantitative Evaluation\\nThe quantitative comparison of our proposed diffCOD with 11 state-\\nof-the-art methods is shown in Table 1. Our method achieves supe-\\nrior performance over other competitors, indicating that our model\\ncan generate high-quality camouflaged segmentation masks com-\\npared to previous methods. For the largest COD10K dataset, our\\nmethod shows a substantial performance jump, with an average in-\\ncrease of 4.8%, 12.8%, 9.5%, 6.4% and 19.1% for SŒ±,Fœâ\\nŒ≤,Fm,\\nEmandMAE , respectively. For another recent large-scale NC4K\\ndataset, diffCOD also outperforms all methods, increasing by 3.4%,\\n(a) Image\\n (b) GT\\n (c) diffCOD\\nFigure 3: Visual results of our proposed model in terms of detailed\\ntextures.\\n7.1%, 6.1%, 4.0% and 14.8% on average for SŒ±,Fœâ\\nŒ≤,Fm,Emand\\nMAE , respectively. In addition, the most significant increases in the\\nCAMO dataset were seen in the Fœâ\\nŒ≤andMAE , with improvements\\nof 10.2% and 11.3%, respectively. CHAMELEON is the smallest\\nCOD dataset, therefore most of the methods perform inconsistently\\non this dataset, our method increases 3.0%, 6.2%, 4.0%, 2.6% and\\n21.2% for SŒ±,Fœâ\\nŒ≤,Fm,EmandMAE , respectively.\\n4.3 Qualitative Evaluation\\nFigure 4 shows a comprehensive visual comparison with current\\nstate-of-the-art methods. It can be found that our method achieves\\ncompetitive visual performance in different types of challenging sce-\\nnarios. Our diffCOD is able to guarantee the integrity and correctness\\n\\n(a)Image\\n (b)GT\\n (c)Ours\\n (d)CRNet [18]\\n (e)CubeNet [63]\\n (f)PFNet [37]\\n (g)SINet [13]\\n (h)MINet [41]\\nFigure 4: Qualitative comparison of our proposed method and other representative COD methods. Our method provides better performance\\nthan all competitors for camouflaged object segmentation in various complex scenes.\\nof recognition even under difficult conditions, such as single object\\n(e.g.,row 1-4), multi-objects ( e.g.,row 5-8), small object ( e.g., row 9-\\n11). Nature‚Äôs camouflaged organisms often have strange traits, such\\nas tentacles, tiny spikes, etc. Past models have blurred the recognition\\nof edge parts even if the location of the target is correctly targeted.\\nHowever, we are surprised by the advantages of diffCOD in terms\\nof detailed textures. As shown in Figure 3, our method is able to\\naccurately identify every subtlety, and it can depict the textures of\\nthe object in extremely fine detail, solving the blurring problem of\\nsegmentation masks in other methods.4.4 Ablation Studies\\nOverview. We perform ablation studies on key components to ver-\\nify their effectiveness and analyze their impacts on performance, as\\nshown in Table 2. Experimental results demonstrate that our designed\\nInjection Attention Module (IAM), Feature Fusion (FF), and ViT can\\nimprove detection performance. When they are combined to build\\ndiffCOD, significant improvements in all evaluation metrics are ob-\\nserved. Note that the Baseline refers to the standard diffusion model.\\n\\nNo.Component COD10K NC4K CAMO\\nBaseline IAM FF ViT SŒ±‚ÜëFœâ\\nŒ≤‚ÜëFm‚ÜëEm‚ÜëMAE ‚ÜìSŒ±‚ÜëFœâ\\nŒ≤‚ÜëFm‚ÜëEm‚ÜëMAE ‚ÜìSŒ±‚ÜëFœâ\\nŒ≤‚ÜëFm‚ÜëEm‚ÜëMAE ‚Üì\\n#1 ‚úì 0.761 0.604 0.657 0.845 0.046 0.781 0.687 0.712 0.841 0.061 0.731 0.607 0.664 0.790 0.097\\n#2 ‚úì ‚úì 0.788 0.638 0.687 0.861 0.041 0.805 0.711 0.747 0.863 0.056 0.749 0.631 0.694 0.805 0.093\\n#3 ‚úì ‚úì ‚úì 0.801 0.662 0.709 0.876 0.039 0.823 0.731 0.772 0.876 0.054 0.770 0.664 0.718 0.829 0.087\\n#4 ‚úì ‚úì ‚úì 0.809 0.677 0.719 0.888 0.036 0.835 0.758 0.798 0.889 0.051 0.792 0.693 0.751 0.849 0.083\\n#5 ‚úì ‚úì ‚úì 0.799 0.657 0.708 0.868 0.039 0.820 0.727 0.770 0.872 0.054 0.772 0.663 0.722 0.831 0.086\\n#OUR ‚úì ‚úì ‚úì ‚úì 0.812 0.684 0.723 0.892 0.036 0.837 0.761 0.802 0.891 0.051 0.795 0.704 0.758 0.852 0.082\\nTable 2: Ablation studies of our diffCOD. The best results are marked in bold .\\n(a)Image\\n (b)GT\\n (c)Sample 1\\n (d)Sample 2\\n (e)Sample 3\\n (f)Sample 4\\n (g)Sample 5\\nFigure 5: Visual results of the sampling process. (c)-(g) is the diffCOD sampling process. The time step is 200, 400, 600, 800, and 1000,\\nrespectively.\\nEffectiveness of IAM. As can be seen in Table 2, the presence or ab-\\nsence of IAM plays a key role in the performance improvement of the\\nmodel. Compared to the experiments without this key component,\\nthe average improvement of #2 with IAM over #1 for SŒ±,Fœâ\\nŒ≤,Fm,\\nEmandMAE on the three datasets is 3.0%, 4.3%, 4.7%, 2.1% and\\n7.7%, respectively. Furthermore, #Our accuracy improvement over\\n#5 is significant, with an average increase of 6.0% in MAE metric\\non the three datasets. This is a good indication that IAM integrates\\ndiffusion features and texture features from the backbone perfectly.\\nEffectiveness of FF. The main role of FF is to aggregate the multi-\\nscale features. As shown in Table 2, compared to No. #2, No. #3 has\\nan average improvement of 2.2%, 5.0%, 3.8%, 2.5% and 6.0% for\\nSŒ±,Fœâ\\nŒ≤,Fm,EmandMAE on the three datasets, respectively. The\\nperformance of #Ours on SŒ±,Fœâ\\nŒ≤,FmandEmis 3.2%, 1.0%, 0.7%\\nand 0.3% higher than that of No. #4.\\nEffectiveness of ViT. To obtain the location information and texture\\ninformation of the objects in the original features, we use a ViT as a\\nbackbone to assist the diffusion process. From Table 2, we can learn\\nthat #Ours containing rich original features has an average improve-\\nment of 2.1%, 4.5%, 3.8%, 2.1% and 6.3% over #3 for SŒ±,Fœâ\\nŒ≤,Fm,\\nEmandMAE on the three datasets, respectively. #2, which con-\\ntains no original features at all, has an average of 4.0%, 7.5%, 6.6%,\\n3.9% and 10.6% lower than #4 for SŒ±,Fœâ\\nŒ≤,Fm,EmandMAE onthe three data sets, respectively. In addition, to further demonstrate\\nthe significance of conditional semantic features to guide the diffu-\\nsion process, we visualize the sampling process of diffCOD. From\\nFigure 5, we can see that our model learns part of the location infor-\\nmation and texture patterns of the camouflaged objects at the early\\nstage of denoising, and the subsequent inference process gradually\\nrefines the final mask by training out the denoising model on this\\nbasis. This shows that the key clues extracted by ViT are perfectly\\nintegrated into the diffusion process with the help of FF and IAM.\\n5 Conclusion\\nIn this paper, we propose a diffusion-based framework for cam-\\nouflaged object detection, which changes the previous detection\\nparadigm of the COD community by using a generative model for\\nthe segmentation of camouflaged objects to achieve significant per-\\nformance gains. To the best of our knowledge, this is the first frame-\\nwork that employs a denoising diffusion model for COD tasks. Our\\napproach decouples the task of segmenting camouflaged objects into\\na series of forward and reverse diffusion processes, and integrates key\\ninformation from conditional semantic features to guide this process.\\nExtensive experiments show the superiority over 11 other state-of-\\nthe-art methods on four datasets. As a new paradigm for camouflaged\\nobject detection, we hope that our proposed method will serve as a\\nsolid baseline and encourage future research.\\n\\nReferences\\n[1] Tomer Amit, Eliya Nachmani, Tal Shaharbany, and Lior Wolf,\\n‚ÄòSegdiff: Image segmentation with diffusion probabilistic models‚Äô,\\narXiv preprint arXiv:2112.00390 , (2021).\\n[2] Eslam Mohamed Bakr, Pengzhan Sun, Xiaoqian Shen, Faizan Farooq\\nKhan, Li Erran Li, and Mohamed Elhoseiny, ‚ÄòHrs-bench: Holistic, reli-\\nable and scalable benchmark for text-to-image models‚Äô, arXiv preprint\\narXiv:2304.05390 , (2023).\\n[3] Dmitry Baranchuk, Andrey V oynov, Ivan Rubachev, Valentin Khrulkov,\\nand Artem Babenko, ‚ÄòLabel-efficient semantic segmentation with dif-\\nfusion models‚Äô, in ICLR , (2022).\\n[4] Emmanuel Asiedu Brempong, Simon Kornblith, Ting Chen, Niki Par-\\nmar, Matthias Minderer, and Mohammad Norouzi, ‚ÄòDenoising pretrain-\\ning for semantic segmentation‚Äô, in CVPR , pp. 4175‚Äì4186, (2022).\\n[5] Xuelian Cheng, Huan Xiong, Deng-Ping Fan, Yiran Zhong, Mehrtash\\nHarandi, Tom Drummond, and Zongyuan Ge, ‚ÄòImplicit motion han-\\ndling for video camouflaged object detection‚Äô, in CVPR , pp. 13864‚Äì\\n13873, (2022).\\n[6] Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye, ‚ÄòCome-closer-\\ndiffuse-faster: Accelerating conditional diffusion models for inverse\\nproblems through stochastic contraction‚Äô, in Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\\npp. 12413‚Äì12422, (2022).\\n[7] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu\\nCord, ‚ÄòDiffedit: Diffusion-based semantic image editing with mask\\nguidance‚Äô, arXiv preprint arXiv:2210.11427 , (2022).\\n[8] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and\\nMubarak Shah, ‚ÄòDiffusion models in vision: A survey‚Äô, IEEE TPAMI ,\\n(2023).\\n[9] Max Daniels, Tyler Maunu, and Paul Hand, ‚ÄòScore-based generative\\nneural networks for large-scale optimal transport‚Äô, NeurIPS ,34, 12955‚Äì\\n12965, (2021).\\n[10] Prafulla Dhariwal and Alexander Nichol, ‚ÄòDiffusion models beat gans\\non image synthesis‚Äô, NeurIPS ,34, 8780‚Äì8794, (2021).\\n[11] Bo Dong, Jialun Pei, Rongrong Gao, Tian-Zhu Xiang, Shuo Wang,\\nand Huan Xiong, ‚ÄòA unified query-based paradigm for camouflaged in-\\nstance segmentation‚Äô, in ACM MM , (2023).\\n[12] Deng-Ping Fan, Ge-Peng Ji, Ming-Ming Cheng, and Ling Shao, ‚ÄòCon-\\ncealed object detection‚Äô, IEEE TPAMI , (2021).\\n[13] Deng-Ping Fan, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng, Jianbing\\nShen, and Ling Shao, ‚ÄòCamouflaged object detection‚Äô, in CVPR , pp.\\n2777‚Äì2787, (2020).\\n[14] Deng-Ping Fan, Ge-Peng Ji, Peng Xu, Ming-Ming Cheng, Christos\\nSakaridis, and Luc Van Gool, ‚ÄòAdvances in deep concealed scene un-\\nderstanding‚Äô, arXiv preprint arXiv:2304.11234 , (2023).\\n[15] Deng-Ping Fan, Ge-Peng Ji, Tao Zhou, Geng Chen, Huazhu Fu, Jian-\\nbing Shen, and Ling Shao, ‚ÄòPranet: Parallel reverse attention network\\nfor polyp segmentation‚Äô, in MICCAI , pp. 263‚Äì273. Springer, (2020).\\n[16] Deng-Ping Fan, Tao Zhou, Ge-Peng Ji, Yi Zhou, Geng Chen, Huazhu\\nFu, Jianbing Shen, and Ling Shao, ‚ÄòInf-net: Automatic covid-19 lung\\ninfection segmentation from ct images‚Äô, IEEE Transactions on Medical\\nImaging ,39(8), 2626‚Äì2637, (2020).\\n[17] Zhangxuan Gu, Haoxing Chen, Zhuoer Xu, Jun Lan, Changhua Meng,\\nand Weiqiang Wang, ‚ÄòDiffusioninst: Diffusion model for instance seg-\\nmentation‚Äô, arXiv preprint arXiv:2212.02773 , (2022).\\n[18] Ruozhen He, Qihua Dong, Jiaying Lin, and Rynson WH Lau, ‚ÄòWeakly-\\nsupervised camouflaged object detection with scribble annotations‚Äô,\\nAAAI , 781‚Äì789, (2023).\\n[19] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch,\\nand Daniel Cohen-Or, ‚ÄòPrompt-to-prompt image editing with cross at-\\ntention control‚Äô, arXiv preprint arXiv:2208.01626 , (2022).\\n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel, ‚ÄòDenoising diffusion prob-\\nabilistic models‚Äô, NeurIPS ,33, 6840‚Äì6851, (2020).\\n[21] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mo-\\nhammad Norouzi, and David J Fleet, ‚ÄòVideo diffusion models‚Äô, arXiv\\npreprint arXiv:2204.03458 , (2022).\\n[22] Xiaobin Hu, Shuo Wang, Xuebin Qin, Hang Dai, Wenqi Ren, Dong-\\nhao Luo, Ying Tai, and Ling Shao, ‚ÄòHigh-resolution iterative feedback\\nnetwork for camouflaged object detection‚Äô, AAAI , (2023).\\n[23] Zhou Huang, Hang Dai, Tian-Zhu Xiang, Shuo Wang, Huai-Xin Chen,\\nJie Qin, and Huan Xiong, ‚ÄòFeature shrinkage pyramid for camouflaged\\nobject detection with transformers‚Äô, CVPR , (2023).\\n[24] Ge-Peng Ji, Deng-Ping Fan, Yu-Cheng Chou, Dengxin Dai, AlexanderLiniger, and Luc Van Gool, ‚ÄòDeep gradient learning for efficient cam-\\nouflaged object detection‚Äô, Machine Intelligence Research , (2023).\\n[25] Ge-Peng Ji, Lei Zhu, Mingchen Zhuge, and Keren Fu, ‚ÄòFast camou-\\nflaged object detection via edge-based reversible re-calibration net-\\nwork‚Äô, Pattern Recognition ,123, 108414, (2022).\\n[26] Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu, Zhaoqiang\\nLiu, Tong Lu, Zhenguo Li, and Ping Luo, ‚ÄòDdp: Diffusion model for\\ndense visual prediction‚Äô, arXiv preprint arXiv:2303.17559 , (2023).\\n[27] Qi Jia, Shuilian Yao, Yu Liu, Xin Fan, Risheng Liu, and Zhongxuan\\nLuo, ‚ÄòSegment, magnify and reiterate: Detecting camouflaged objects\\nthe hard way‚Äô, in CVPR , pp. 4713‚Äì4722, (2022).\\n[28] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Bo-\\nqing Gong, Tingbo Hou, Huisheng Wang, and Yu-Chuan Su, ‚ÄòTaming\\nencoder for zero fine-tuning image customization with text-to-image\\ndiffusion models‚Äô, arXiv preprint arXiv:2304.02642 , (2023).\\n[29] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine, ‚ÄòElucidating\\nthe design space of diffusion-based generative models‚Äô, arXiv preprint\\narXiv:2206.00364 , (2022).\\n[30] Karthika Suresh Kumar and Aamer Abdul Rahman, ‚ÄòEarly detection of\\nlocust swarms using deep learning‚Äô, in Advances in machine learning\\nand computational intelligence , 303‚Äì310, Springer, (2021).\\n[31] Trung-Nghia Le, Tam V Nguyen, Zhongliang Nie, Minh-Triet Tran,\\nand Akihiro Sugimoto, ‚ÄòAnabranch network for camouflaged object\\nsegmentation‚Äô, CVIU ,184, 45‚Äì56, (2019).\\n[32] Aixuan Li, Jing Zhang, Yunqiu Lv, Bowen Liu, Tong Zhang, and\\nYuchao Dai, ‚ÄòUncertainty-aware joint salient object and camouflaged\\nobject detection‚Äô, in CVPR , pp. 10071‚Äì10081, (2021).\\n[33] Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun Feng, Zhi-\\nhai Xu, Qi Li, and Yueting Chen, ‚ÄòSrdiff: Single image super-resolution\\nwith diffusion probabilistic models‚Äô, Neurocomputing ,479, 47‚Äì59,\\n(2022).\\n[34] Lin Li, Jingyi Liu, Shuo Wang, Xunkun Wang, and Tian-Zhu Xiang,\\n‚ÄòTrichomonas vaginalis segmentation in microscope images‚Äô, in MIC-\\nCAI, pp. 68‚Äì78. Springer, (2022).\\n[35] Lin Li, Jingyi Liu, Fei Yu, Xunkun Wang, and Tian-Zhu Xiang,\\n‚ÄòMvdi25k: A large-scale dataset of microscopic vaginal discharge\\nimages‚Äô, BenchCouncil Transactions on Benchmarks, Standards and\\nEvaluations ,1(1), 100008, (2021).\\n[36] Yunqiu Lv, Jing Zhang, Yuchao Dai, Aixuan Li, Bowen Liu, Nick\\nBarnes, and Deng-Ping Fan, ‚ÄòSimultaneously localize, segment and\\nrank the camouflaged objects‚Äô, in CVPR , pp. 11591‚Äì11601, (2021).\\n[37] Haiyang Mei, Ge-Peng Ji, Ziqi Wei, Xin Yang, Xiaopeng Wei, and\\nDeng-Ping Fan, ‚ÄòCamouflaged object segmentation with distraction\\nmining‚Äô, in CVPR , pp. 8772‚Äì8781, (2021).\\n[38] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and\\nStefano Ermon, ‚ÄòSdedit: Image synthesis and editing with stochastic\\ndifferential equations‚Äô, arXiv preprint arXiv:2108.01073 , (2021).\\n[39] Alexander Quinn Nichol and Prafulla Dhariwal, ‚ÄòImproved denoising\\ndiffusion probabilistic models‚Äô, in ICML , pp. 8162‚Äì8171, (2021).\\n[40] Youwei Pang, Xiaoqi Zhao, Tian-Zhu Xiang, Lihe Zhang, and Huchuan\\nLu, ‚ÄòZoom in and out: A mixed-scale triplet network for camouflaged\\nobject detection‚Äô, in CVPR , pp. 2160‚Äì2170, (2022).\\n[41] Youwei Pang, Xiaoqi Zhao, Lihe Zhang, and Huchuan Lu, ‚ÄòMulti-scale\\ninteractive network for salient object detection‚Äô, in CVPR , pp. 9413‚Äì\\n9422, (2020).\\n[42] Aditya Prakash, Kashyap Chitta, and Andreas Geiger, ‚ÄòMulti-modal\\nfusion transformer for end-to-end autonomous driving‚Äô, in CVPR , pp.\\n7077‚Äì7087, (2021).\\n[43] Aimon Rahman, Jeya Maria Jose Valanarasu, Ilker Hacihaliloglu, and\\nVishal M Patel, ‚ÄòAmbiguous medical image segmentation using diffu-\\nsion models‚Äô, in CVPR , (2023).\\n[44] Olaf Ronneberger, Philipp Fischer, and Thomas Brox, ‚ÄòU-net: Convo-\\nlutional networks for biomedical image segmentation‚Äô, in MICCAI , pp.\\n234‚Äì241. Springer, (2015).\\n[45] Przemys≈Çaw Skurowski, Hassan Abdulameer, J B≈Çaszczyk, Tomasz\\nDepta, Adam Kornacki, and P Kozie≈Ç, ‚ÄòAnimal camouflage analysis:\\nChameleon database‚Äô, Unpublished manuscript ,2(6), 7, (2018).\\n[46] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya\\nGanguli, ‚ÄòDeep unsupervised learning using nonequilibrium thermody-\\nnamics‚Äô, in International Conference on Machine Learning , pp. 2256‚Äì\\n2265. PMLR, (2015).\\n[47] Yang Song and Stefano Ermon, ‚ÄòGenerative modeling by estimating\\ngradients of the data distribution‚Äô, NeurIPS ,32, (2019).\\n[48] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek\\n\\nKumar, Stefano Ermon, and Ben Poole, ‚ÄòScore-based generative\\nmodeling through stochastic differential equations‚Äô, arXiv preprint\\narXiv:2011.13456 , (2020).\\n[49] Yujia Sun, Geng Chen, Tao Zhou, Yi Zhang, and Nian Liu, ‚ÄòContext-\\naware cross-level fusion network for camouflaged object detection‚Äô, IJ-\\nCAI, 1025‚Äì1031, (2021).\\n[50] Yujia Sun, Shuo Wang, Chenglizhao Chen, and Tian-Zhu Xiang,\\n‚ÄòBoundary-guided camouflaged object detection‚Äô, IJCAI , 1335‚Äì1341,\\n(2022).\\n[51] Domen Tabernik, Samo ≈†ela, Jure Skvar Àác, and Danijel Sko Àácaj,\\n‚ÄòSegmentation-based deep-learning approach for surface-defect detec-\\ntion‚Äô, Journal of Intelligent Manufacturing ,31(3), 759‚Äì776, (2020).\\n[52] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding\\nLiang, Tong Lu, Ping Luo, and Ling Shao, ‚ÄòPvtv2: Improved baselines\\nwith pyramid vision transformer‚Äô, Computational Visual Media ,8(3),\\n1‚Äì10, (2022).\\n[53] Junde Wu, Huihui Fang, Yu Zhang, Yehui Yang, and Yanwu Xu,\\n‚ÄòMedsegdiff: Medical image segmentation with diffusion probabilistic\\nmodel‚Äô, MIDL , (2023).\\n[54] Junde Wu, Rao Fu, Huihui Fang, Yu Zhang, and Yanwu Xu,\\n‚ÄòMedsegdiff-v2: Diffusion based medical image segmentation with\\ntransformer‚Äô, arXiv preprint arXiv:2301.11798 , (2023).\\n[55] Weijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou, and Chun-\\nhua Shen, ‚ÄòDiffumask: Synthesizing images with pixel-level annota-\\ntions for semantic segmentation using diffusion models‚Äô, ICCV , (2023).\\n[56] Zhe Wu, Li Su, and Qingming Huang, ‚ÄòCascaded partial decoder for\\nfast and accurate salient object detection‚Äô, in CVPR , pp. 3907‚Äì3916,\\n(2019).\\n[57] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang,\\nand Shalini De Mello, ‚ÄòOpen-vocabulary panoptic segmentation with\\ntext-to-image diffusion models‚Äô, CVPR , (2023).\\n[58] Bowen Yin, Xuying Zhang, Qibin Hou, Bo-Yuan Sun, Deng-Ping Fan,\\nand Luc Van Gool, ‚ÄòCamoformer: Masked separable attention for cam-\\nouflaged object detection‚Äô, arXiv preprint arXiv:2212.06570 , (2022).\\n[59] Qiang Zhai, Xin Li, Fan Yang, Chenglizhao Chen, Hong Cheng, and\\nDeng-Ping Fan, ‚ÄòMutual graph learning for camouflaged object detec-\\ntion‚Äô, in CVPR , pp. 12997‚Äì13007, (2021).\\n[60] Cong Zhang, Kang Wang, Hongbo Bi, Ziqi Liu, and Lina Yang, ‚ÄòCam-\\nouflaged object detection via neighbor connection and hierarchical in-\\nformation transfer‚Äô, CVIU ,221, 103450, (2022).\\n[61] Miao Zhang, Shuang Xu, Yongri Piao, Dongxiang Shi, Shusen Lin, and\\nHuchuan Lu, ‚ÄòPreynet: Preying on camouflaged objects‚Äô, in ACM MM ,\\npp. 5323‚Äì5332, (2022).\\n[62] Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao, Jufeng\\nYang, and Ming-Ming Cheng, ‚ÄòEgnet: Edge guidance network for\\nsalient object detection‚Äô, in ICCV , pp. 8778‚Äì8787, (October 2019).\\n[63] Yijie Zhong, Bo Li, Lv Tang, Senyun Kuang, Shuang Wu, and\\nShouhong Ding, ‚ÄòDetecting camouflaged object in frequency domain‚Äô,\\ninCVPR , pp. 4504‚Äì4513, (2022).\\n[64] Hongwei Zhu, Peng Li, Haoran Xie, Xuefeng Yan, Dong Liang,\\nDapeng Chen, Mingqiang Wei, and Jing Qin, ‚ÄòI can find you! boundary-\\nguided separated attention network for camouflaged object detection‚Äô,\\ninAAAI , pp. 3608‚Äì3616, (2022).\\n[65] Jinchao Zhu, Xiaoyu Zhang, Shuo Zhang, and Junnan Liu, ‚ÄòInferring\\ncamouflaged objects by texture-aware interactive guidance network‚Äô,\\ninAAAI , pp. 3599‚Äì3607, (2021).\\n[66] Mingchen Zhuge, Xiankai Lu, Yiyou Guo, Zhihua Cai, and Shuhan\\nChen, ‚ÄòCubenet: X-shape connection for camouflaged object detection‚Äô,\\nPattern Recognition ,127, 108644, (2022).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_arxiv_objects[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumuk\\anaconda3\\envs\\ResearchSurveyLLM\\lib\\site-packages\\weaviate\\__init__.py:128: DeprecationWarning: Dep010: Importing AuthApiKey from weaviate is deprecated. Please import it from its specific module: weaviate.auth\n",
      "  _Warnings.root_module_import(name, map_[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mget_build_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaper_arxiv_objects\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 12\u001b[0m, in \u001b[0;36mget_build_index\u001b[1;34m(documents, embed_model, sentence_window_size, save_dir)\u001b[0m\n\u001b[0;32m      8\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m WeaviateVectorStore(weaviate_client \u001b[38;5;241m=\u001b[39m client, index_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArXivPaper\u001b[39m\u001b[38;5;124m\"\u001b[39m, text_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m storage_context \u001b[38;5;241m=\u001b[39m StorageContext\u001b[38;5;241m.\u001b[39mfrom_defaults(vector_store \u001b[38;5;241m=\u001b[39m vector_store)\n\u001b[1;32m---> 12\u001b[0m sentence_context \u001b[38;5;241m=\u001b[39m \u001b[43mServiceContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43membed_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode_parser\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnode_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(save_dir):\n\u001b[0;32m     19\u001b[0m       \u001b[38;5;66;03m# create and load the index\u001b[39;00m\n\u001b[0;32m     20\u001b[0m       index \u001b[38;5;241m=\u001b[39m VectorStoreIndex\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[0;32m     21\u001b[0m           documents, service_context\u001b[38;5;241m=\u001b[39msentence_context, storage_context \u001b[38;5;241m=\u001b[39m storage_context\n\u001b[0;32m     22\u001b[0m       )\n",
      "File \u001b[1;32mc:\\Users\\sumuk\\anaconda3\\envs\\ResearchSurveyLLM\\lib\\site-packages\\llama_index\\service_context.py:191\u001b[0m, in \u001b[0;36mServiceContext.from_defaults\u001b[1;34m(cls, llm_predictor, llm, prompt_helper, embed_model, node_parser, text_splitter, transformations, llama_logger, callback_manager, system_prompt, query_wrapper_prompt, pydantic_program_mode, chunk_size, chunk_overlap, context_window, num_output, chunk_size_limit)\u001b[0m\n\u001b[0;32m    186\u001b[0m         llm_predictor\u001b[38;5;241m.\u001b[39mquery_wrapper_prompt \u001b[38;5;241m=\u001b[39m query_wrapper_prompt\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# NOTE: the embed_model isn't used in all indices\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# NOTE: embed model should be a transformation, but the way the service\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# context works, we can't put in there yet.\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m embed_model \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_embed_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m embed_model\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m callback_manager\n\u001b[0;32m    194\u001b[0m prompt_helper \u001b[38;5;241m=\u001b[39m prompt_helper \u001b[38;5;129;01mor\u001b[39;00m _get_default_prompt_helper(\n\u001b[0;32m    195\u001b[0m     llm_metadata\u001b[38;5;241m=\u001b[39mllm_predictor\u001b[38;5;241m.\u001b[39mmetadata,\n\u001b[0;32m    196\u001b[0m     context_window\u001b[38;5;241m=\u001b[39mcontext_window,\n\u001b[0;32m    197\u001b[0m     num_output\u001b[38;5;241m=\u001b[39mnum_output,\n\u001b[0;32m    198\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\sumuk\\anaconda3\\envs\\ResearchSurveyLLM\\lib\\site-packages\\llama_index\\embeddings\\utils.py:84\u001b[0m, in \u001b[0;36mresolve_embed_model\u001b[1;34m(embed_model)\u001b[0m\n\u001b[0;32m     80\u001b[0m         embed_model \u001b[38;5;241m=\u001b[39m InstructorEmbedding(\n\u001b[0;32m     81\u001b[0m             model_name\u001b[38;5;241m=\u001b[39mmodel_name, cache_folder\u001b[38;5;241m=\u001b[39mcache_folder\n\u001b[0;32m     82\u001b[0m         )\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m         embed_model \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LCEmbeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embed_model, LCEmbeddings):\n\u001b[0;32m     89\u001b[0m     embed_model \u001b[38;5;241m=\u001b[39m LangchainEmbedding(embed_model)\n",
      "File \u001b[1;32mc:\\Users\\sumuk\\anaconda3\\envs\\ResearchSurveyLLM\\lib\\site-packages\\llama_index\\embeddings\\huggingface.py:88\u001b[0m, in \u001b[0;36mHuggingFaceEmbedding.__init__\u001b[1;34m(self, model_name, tokenizer_name, pooling, max_length, query_instruction, text_instruction, normalize, model, tokenizer, embed_batch_size, cache_folder, trust_remote_code, device, callback_manager)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Extract model_name from model\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mname_or_path\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Use tokenizer_name with AutoTokenizer\u001b[39;00m\n\u001b[0;32m     91\u001b[0m     tokenizer_name \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     92\u001b[0m         model_name \u001b[38;5;129;01mor\u001b[39;00m tokenizer_name \u001b[38;5;129;01mor\u001b[39;00m DEFAULT_HUGGINGFACE_EMBEDDING_MODEL\n\u001b[0;32m     93\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\sumuk\\anaconda3\\envs\\ResearchSurveyLLM\\lib\\site-packages\\transformers\\modeling_utils.py:2595\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   2591\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2592\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2593\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2594\u001b[0m         )\n\u001b[1;32m-> 2595\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sumuk\\anaconda3\\envs\\ResearchSurveyLLM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sumuk\\anaconda3\\envs\\ResearchSurveyLLM\\lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sumuk\\anaconda3\\envs\\ResearchSurveyLLM\\lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sumuk\\anaconda3\\envs\\ResearchSurveyLLM\\lib\\site-packages\\torch\\nn\\modules\\module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\sumuk\\anaconda3\\envs\\ResearchSurveyLLM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "index = get_build_index(paper_arxiv_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ResearchSurveyLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
