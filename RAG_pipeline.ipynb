{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kganapa/ResearchLLM/resllm/lib/python3.10/site-packages/llama_index/vector_stores/lancedb.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  from pandas import DataFrame\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import json\n",
    "\n",
    "import os\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "from llama_index import VectorStoreIndex, ServiceContext\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index import Document\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.vector_stores import WeaviateVectorStore\n",
    "from llama_index import VectorStoreIndex, StorageContext\n",
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "from llama_index.postprocessor import MetadataReplacementPostProcessor, SentenceTransformerRerank\n",
    "from llama_index.response.notebook_utils import display_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /home/kganapa/ResearchLLM/Mistral_7b/mistral-7b-instruct-v0.2.Q4_K_M (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    16.02 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   308.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Using chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: \n",
      "Using chat bos_token: \n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    # model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf',\n",
    "    # model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf',\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=\"/home/kganapa/ResearchLLM/Mistral_7b/mistral-7b-instruct-v0.2.Q4_K_M\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=4096,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    input_files = [\"/home/kganapa/ResearchLLM/data/HGNN_General_Hypergraph_Neural_Networks.pdf\"]\n",
    ").load_data()\n",
    "\n",
    "documents = Document(text = \"\\n\\n\".join([doc.text for doc in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kganapa/ResearchLLM/resllm/lib/python3.10/site-packages/weaviate/warnings.py:158: DeprecationWarning: Dep016: You are using the Weaviate v3 client, which is deprecated.\n",
      "            Consider upgrading to the new and improved v4 client instead!\n",
      "            See here for usage: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "client = weaviate.Client(\n",
    "    url = \"http://localhost:8080\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnexpectedStatusCodeError",
     "evalue": "Create class! Unexpected status code: 422, with response body: {'error': [{'message': 'class name \"ArXivPaper\" already exists'}]}.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusCodeError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 41\u001b[0m\n\u001b[1;32m      1\u001b[0m schema \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m      3\u001b[0m                 {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m             ]\n\u001b[1;32m     40\u001b[0m          }\n\u001b[0;32m---> 41\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ResearchLLM/resllm/lib/python3.10/site-packages/weaviate/schema/crud_schema.py:201\u001b[0m, in \u001b[0;36mSchema.create\u001b[0;34m(self, schema)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03mCreate the schema of the Weaviate instance, with all classes at once.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m    If the 'schema' could not be validated against the standard format.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m loaded_schema \u001b[38;5;241m=\u001b[39m _get_dict_from_object(schema)\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_classes_with_primitives\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_schema\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclasses\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_complex_properties_from_classes(loaded_schema[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/ResearchLLM/resllm/lib/python3.10/site-packages/weaviate/schema/crud_schema.py:832\u001b[0m, in \u001b[0;36mSchema._create_classes_with_primitives\u001b[0;34m(self, schema_classes_list)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;124;03mCreate all the classes in the list and primitive properties.\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;124;03mThis function does not create references,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;124;03m    A list of classes as they are found in a schema JSON description.\u001b[39;00m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m weaviate_class \u001b[38;5;129;01min\u001b[39;00m schema_classes_list:\n\u001b[0;32m--> 832\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_class_with_primitives\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweaviate_class\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ResearchLLM/resllm/lib/python3.10/site-packages/weaviate/schema/crud_schema.py:817\u001b[0m, in \u001b[0;36mSchema._create_class_with_primitives\u001b[0;34m(self, weaviate_class)\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsConnectionError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass may not have been created properly.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconn_err\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m--> 817\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedStatusCodeException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate class\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n",
      "\u001b[0;31mUnexpectedStatusCodeError\u001b[0m: Create class! Unexpected status code: 422, with response body: {'error': [{'message': 'class name \"ArXivPaper\" already exists'}]}."
     ]
    }
   ],
   "source": [
    "schema = {\n",
    "            \"classes\": [\n",
    "                {\n",
    "                \"class\": \"ArXivPaper\",\n",
    "                \"description\": \"Represents a paper from ArXiv\",\n",
    "                \"properties\": [\n",
    "                    {\n",
    "                    \"name\": \"title\",\n",
    "                    \"dataType\": [\"string\"],\n",
    "                    \"description\": \"Title of the paper\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"name\": \"authors\",\n",
    "                    \"dataType\": [\"string\"],\n",
    "                    \"description\": \"Authors of the paper\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"name\": \"abstract\",\n",
    "                    \"dataType\": [\"string\"],\n",
    "                    \"description\": \"Abstract of the paper\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"name\": \"content\",\n",
    "                    \"dataType\": [\"text\"],\n",
    "                    \"description\": \"Full text content of the paper\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"name\": \"url\",\n",
    "                    \"dataType\": [\"string\"],\n",
    "                    \"description\": \"URL of the paper on ArXiv\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"name\": \"published_date\",\n",
    "                    \"dataType\": [\"date\"],\n",
    "                    \"description\": \"Date when the paper was published on ArXiv\"\n",
    "                    }\n",
    "                ]\n",
    "                }\n",
    "            ]\n",
    "         }\n",
    "client.schema.create(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_build_index(documents,llm,embed_model=\"local:/home/kganapa/ResearchLLM/UAE-Large-V1\",sentence_window_size=3,save_dir=\"./vector_store/index\"):\n",
    "  \n",
    "  node_parser = SentenceWindowNodeParser(\n",
    "      window_size = sentence_window_size,\n",
    "      window_metadata_key = \"window\",\n",
    "      original_text_metadata_key = \"original_text\"\n",
    "  )\n",
    "\n",
    "  vector_store = WeaviateVectorStore(weaviate_client = client, index_name=\"ArXivPaper\", text_key=\"content\")\n",
    "  storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
    "\n",
    "\n",
    "  sentence_context = ServiceContext.from_defaults(\n",
    "      llm = llm,\n",
    "      embed_model= embed_model,\n",
    "      node_parser = node_parser,\n",
    "  )\n",
    "\n",
    "  if not os.path.exists(save_dir):\n",
    "        # create and load the index\n",
    "        index = VectorStoreIndex.from_documents(\n",
    "            [documents], service_context=sentence_context, storage_context = storage_context\n",
    "        )\n",
    "        index.storage_context.persist(persist_dir=save_dir)\n",
    "  else:\n",
    "      # load the existing index\n",
    "      index = load_index_from_storage(\n",
    "          StorageContext.from_defaults(persist_dir=save_dir),\n",
    "          service_context=sentence_context,\n",
    "      )\n",
    "\n",
    "  return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kganapa/ResearchLLM/resllm/lib/python3.10/site-packages/weaviate/__init__.py:128: DeprecationWarning: Dep010: Importing AuthApiKey from weaviate is deprecated. Please import it from its specific module: weaviate.auth\n",
      "  _Warnings.root_module_import(name, map_[name])\n",
      "/home/kganapa/ResearchLLM/resllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Pooling config file not found; pooling mode is defaulted to 'cls'.\n"
     ]
    }
   ],
   "source": [
    "# get the vector index\n",
    "vector_index = get_build_index(documents=documents, llm=llm, embed_model=\"local:/home/kganapa/ResearchLLM/UAE-Large-V1\", sentence_window_size=3, save_dir=\"./vector_store/index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_engine(sentence_index, similarity_top_k=6, rerank_top_n=2):\n",
    "  postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "  rerank = SentenceTransformerRerank(\n",
    "      top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "  )\n",
    "  engine = sentence_index.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank]\n",
    "  )\n",
    "\n",
    "  return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = get_query_engine(sentence_index=vector_index, similarity_top_k=6, rerank_top_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   37833.53 ms\n",
      "llama_print_timings:      sample time =      34.06 ms /   145 runs   (    0.23 ms per token,  4256.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =   58363.21 ms /   682 tokens (   85.58 ms per token,    11.69 tokens per second)\n",
      "llama_print_timings:        eval time =   13394.84 ms /   144 runs   (   93.02 ms per token,    10.75 tokens per second)\n",
      "llama_print_timings:       total time =   72064.26 ms /   826 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The hypergraph convolutions mentioned in the context are two specific types of convolution layers used in Hypergraph Neural Networks (HGNNs). The first one is called HGNNConv, which is defined from the spectral aspect. The second one is called HGNNConv\\_spatial, which is defined from the spatial aspect. The spectral aspect refers to the use of graph spectral theory to design the convolution operation, while the spatial aspect refers to the use of the spatial structure of the hypergraph to define the convolution operation. The text describes the spatial aspect of the hypergraph convolution in more detail, specifically a two-stage message passing framework for spatial hypergraph convolution."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query=\"What are the hypergraph convolutions mentioned?\"\n",
    "response = query_engine.query(query)\n",
    "display_response(response)\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resllm",
   "language": "python",
   "name": "resllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
